#!/usr/bin/env python3
"""
CEFR Word List Batch Translator
Haszn√°lat: python translate_cefr.py
"""

import google.generativeai as genai
import csv
import json
import time
from pathlib import Path

# ============================================
# KONFIGUR√ÅCI√ì
# ============================================
API_KEY = "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"
INPUT_CSV = "../public/data/word_list_cefr.csv"  # Relat√≠v √∫tvonal a CSV-hez
OUTPUT_JSON = "../public/data/cefr_dictionary.json"  # Output k√∂zvetlen√ºl a Next.js data mapp√°ba
BATCH_SIZE = 30  # ‚¨ÖÔ∏è Cs√∂kkentve 30-r√≥l 10-re (kevesebb k√©r√©s)
DELAY_BETWEEN_BATCHES = 3  # ‚¨ÖÔ∏è N√∂velve 3-r√≥l 10-re (lassabb tempo)

# üß™ TESZT M√ìD - csak az els≈ë 50 sz√≥t ford√≠tja le
TEST_MODE = False  # ‚¨ÖÔ∏è False = √ñSSZES sz√≥ (~8000), True = csak 50
TEST_WORD_LIMIT = 50

# ============================================
# GOOGLE GEMINI SETUP
# ============================================
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('models/gemini-2.5-flash')  # M≈±k√∂d≈ë modell!

# ============================================
# FORD√çT√ì F√úGGV√âNY
# ============================================
def translate_word_batch(words_batch):
    """
    Batch ford√≠t√°s - egyszerre t√∂bb sz√≥
    Returns: dict {"word": ["magyar1", "magyar2", ...], ...}
    """
    words_text = ", ".join(words_batch)
    
    prompt = f"""Te egy professzion√°lis angol-magyar ford√≠t√≥ vagy.

Feladat: Ford√≠tsd le ezeket az angol szavakat magyarra.
Minden sz√≥hoz adj meg 2-5 leggyakoribb magyar jelent√©st.

KRITIKUS SZAB√ÅLYOK:
1. CSAK √©kezetes magyar karaktereket haszn√°lj (√°, √©, √≠, √≥, √∂, ≈ë, √∫, √º, ≈±)
2. SOHA ne haszn√°lj √£, √É, vagy m√°s nem-magyar karaktereket
3. Minden jelent√©s legyen KISBET≈∞S (kiv√©ve tulajdonnevek)
4. R√∂vid, t√∂m√∂r ford√≠t√°sok (1-2 sz√≥ max, kiv√©ve kifejez√©sek)

Form√°tum (JSON):
{{
  "work": ["munka", "dolgozni", "m≈±k√∂dik"],
  "world": ["vil√°g", "f√∂ld"],
  "example": ["p√©lda", "minta"]
}}

Ford√≠tand√≥ szavak: {words_text}

CSAK a JSON-t add vissza, semmi m√°st! Ne √≠rj markdown code block-okat!
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # Clean up markdown code blocks if present
        if result_text.startswith('```json'):
            result_text = result_text[7:]
        elif result_text.startswith('```'):
            result_text = result_text[3:]
        
        if result_text.endswith('```'):
            result_text = result_text[:-3]
        
        result_text = result_text.strip()
        
        # Parse JSON
        translations = json.loads(result_text)
        
        # Validate and clean results
        cleaned = {}
        for word, meanings in translations.items():
            if isinstance(meanings, list):
                # Remove empty strings and limit to 5 meanings
                cleaned[word.lower()] = [m.strip() for m in meanings if m.strip()][:5]
            elif isinstance(meanings, str):
                cleaned[word.lower()] = [meanings.strip()]
        
        return cleaned
        
    except json.JSONDecodeError as e:
        print(f"  ‚ö†Ô∏è  JSON parse hiba: {e}")
        print(f"  Raw response: {result_text[:200]}...")
        return {}
    except Exception as e:
        print(f"  ‚ùå API hiba: {e}")
        return {}

# ============================================
# FALLBACK: Egyedi sz√≥ ford√≠t√°sa
# ============================================
def translate_single_word(word):
    """Ha batch fail, pr√≥b√°ljuk egyenk√©nt"""
    try:
        result = translate_word_batch([word])
        return result.get(word.lower(), ["(ford√≠t√°si hiba)"])
    except:
        return ["(ford√≠t√°si hiba)"]

# ============================================
# CSV BEOLVAS√ÅS - STRUKTUR√ÅLT
# ============================================
def load_words_from_csv(csv_path):
    """
    Load words from CEFR CSV file with metadata
    Returns: list of dicts with word, pos, cefr info
    CSV format: word;pos;CEFR;...
    """
    word_entries = []
    
    if not Path(csv_path).exists():
        print(f"‚ùå Hiba: {csv_path} nem tal√°lhat√≥!")
        print(f"   Jelenlegi mappa: {Path.cwd()}")
        return []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=';')
        
        # Skip header
        header = next(reader, None)
        print(f"üìÑ CSV header: {header}")
        
        for row in reader:
            if row and len(row) >= 3:
                word = row[0].strip()
                pos = row[1].strip() if len(row) > 1 else ""
                cefr = row[2].strip() if len(row) > 2 else ""
                
                if word and cefr:  # Must have word and CEFR level
                    word_entries.append({
                        'word': word,
                        'pos': pos,
                        'cefr': cefr
                    })
    
    return word_entries

# ============================================
# MAIN BATCH PROCESSING
# ============================================
def main():
    print("=" * 60)
    print("üöÄ CEFR BATCH TRANSLATOR - Google Gemini")
    print("=" * 60)
    print()
    
    # Load word entries with metadata
    print(f"üìö CSV beolvas√°s: {INPUT_CSV}")
    word_entries = load_words_from_csv(INPUT_CSV)
    
    if not word_entries:
        print("‚ùå Nincs mit ford√≠tani!")
        return
    
    print(f"‚úÖ √ñsszesen {len(word_entries)} bejegyz√©s bet√∂ltve")
    
    # Group by unique words for batch translation
    unique_words = {}
    for entry in word_entries:
        word = entry['word'].lower()
        if word not in unique_words:
            unique_words[word] = []
        unique_words[word].append(entry)
    
    print(f"üìñ Egyedi szavak: {len(unique_words)}")
    print()
    
    # Load existing dictionary if exists
    dictionary = {}
    if Path(OUTPUT_JSON).exists():
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                dictionary = json.load(f)
            print(f"üìñ Megl√©v≈ë sz√≥t√°r bet√∂ltve: {len(dictionary)} sz√≥")
        except:
            print("‚ö†Ô∏è  Megl√©v≈ë sz√≥t√°r nem olvashat√≥, √∫j kezd√©se...")
    
    # Filter out already translated words
    words_to_translate = [w for w in unique_words.keys() if w not in dictionary]
    
    # üß™ TESZT M√ìD - csak els≈ë 50 sz√≥
    if TEST_MODE:
        words_to_translate = words_to_translate[:TEST_WORD_LIMIT]
        print(f"üß™ TESZT M√ìD: Csak {len(words_to_translate)} sz√≥t ford√≠tunk!")
    
    print(f"üîÑ Ford√≠tand√≥ szavak: {len(words_to_translate)}")
    print()
    
    if len(words_to_translate) == 0:
        print("‚úÖ Minden sz√≥ m√°r le van ford√≠tva!")
        return
    
    # Batch processing - translate words only (not entries)
    total_batches = (len(words_to_translate) + BATCH_SIZE - 1) // BATCH_SIZE
    
    for i in range(0, len(words_to_translate), BATCH_SIZE):
        batch_words = words_to_translate[i:i+BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        
        print(f"üì¶ Batch {batch_num}/{total_batches} - {len(batch_words)} sz√≥")
        print(f"   Szavak: {', '.join(batch_words[:5])}{'...' if len(batch_words) > 5 else ''}")
        
        try:
            # Translate batch (just the words, not metadata)
            translations = translate_word_batch(batch_words)
            
            if translations:
                # Add translations with full entry info
                for word in batch_words:
                    if word in translations:
                        # Get all entries for this word
                        entries = unique_words[word]
                        
                        # Create structured entries with meanings
                        word_data = []
                        for entry in entries:
                            word_data.append({
                                'meanings': translations[word],
                                'pos': entry['pos'],
                                'cefr': entry['cefr']
                            })
                        
                        dictionary[word] = word_data
                
                success_count = len(translations)
                print(f"   ‚úÖ Sikeresen ford√≠tva: {success_count}/{len(batch_words)} sz√≥")
                
                # Handle failed words in batch
                failed_words = [w for w in batch_words if w not in translations]
                if failed_words:
                    print(f"   üîÑ Egyedi pr√≥b√°lkoz√°s: {len(failed_words)} sz√≥...")
                    for word in failed_words:
                        meanings = translate_single_word(word)
                        entries = unique_words[word]
                        word_data = []
                        for entry in entries:
                            word_data.append({
                                'meanings': meanings,
                                'pos': entry['pos'],
                                'cefr': entry['cefr']
                            })
                        dictionary[word] = word_data
                        time.sleep(1)
                
            else:
                print(f"   ‚ö†Ô∏è  Batch fail! Egyedi pr√≥b√°lkoz√°s...")
                for word in batch_words:
                    meanings = translate_single_word(word)
                    entries = unique_words[word]
                    word_data = []
                    for entry in entries:
                        word_data.append({
                            'meanings': meanings,
                            'pos': entry['pos'],
                            'cefr': entry['cefr']
                        })
                    dictionary[word] = word_data
                    time.sleep(1)
            
            # Auto-save after each batch
            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(dictionary, f, ensure_ascii=False, indent=2)
            
            print(f"   üíæ Mentve! √ñsszesen: {len(dictionary)} sz√≥ a sz√≥t√°rban")
            
        except Exception as e:
            print(f"   ‚ùå Hiba t√∂rt√©nt: {e}")
        
        print()
        
        # Rate limit protection
        if batch_num < total_batches:
            time.sleep(DELAY_BETWEEN_BATCHES)
    
    # Final summary
    print("=" * 60)
    print("üéâ FORD√çT√ÅS BEFEJEZVE!")
    print("=" * 60)
    print(f"‚úÖ √ñsszesen {len(dictionary)} egyedi sz√≥ leford√≠tva")
    print(f"üìä √ñsszes bejegyz√©s: {len(word_entries)}")
    print(f"üìÅ F√°jl: {OUTPUT_JSON}")
    print()
    
    # Show sample translations
    print("üìã P√©lda ford√≠t√°sok (strukt√∫r√°val):")
    sample_words = list(dictionary.keys())[:3]
    for word in sample_words:
        print(f"\n  {word}:")
        for entry in dictionary[word]:
            meanings = ', '.join(entry['meanings'])
            print(f"    - {entry['pos']} ({entry['cefr']}): {meanings}")
    
    print()
    print("‚ú® K√©sz! Most m√°r haszn√°lhatod a Next.js appban!")


# ============================================
# RUN
# ============================================
if __name__ == "__main__":
    main()